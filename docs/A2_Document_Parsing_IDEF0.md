# A2 æ–‡ä»¶è§£ææ¨¡çµ„ IDEF0 è©³ç´°è¨­è¨ˆ

## æ–‡ä»¶è³‡è¨Š
- **æ¨¡çµ„ç·¨è™Ÿ**: A2
- **æ¨¡çµ„åç¨±**: æ–‡ä»¶è§£æ
- **è‹±æ–‡åç¨±**: Document Parsing
- **ç‰ˆæœ¬**: v1.0
- **å»ºç«‹æ—¥æœŸ**: 2025-10-30
- **çˆ¶æ¨¡çµ„**: A0 - å°ˆåˆ©æ–‡ä»¶è‡ªå‹•ç”Ÿæˆç³»çµ±

---

## ç›®éŒ„
1. [æ¨¡çµ„æ¦‚è¿°](#æ¨¡çµ„æ¦‚è¿°)
2. [A2-0: æƒ…å¢ƒåœ–](#a2-0-æƒ…å¢ƒåœ–)
3. [A2: é ‚å±¤åŠŸèƒ½åˆ†è§£](#a2-é ‚å±¤åŠŸèƒ½åˆ†è§£)
4. [å­åŠŸèƒ½è©³ç´°è¨­è¨ˆ](#å­åŠŸèƒ½è©³ç´°è¨­è¨ˆ)
5. [è§£ææ¼”ç®—æ³•æµç¨‹](#è§£ææ¼”ç®—æ³•æµç¨‹)
6. [JSON Schema å®šç¾©](#json-schema-å®šç¾©)
7. [å“è³ªé©—è­‰è¦å‰‡](#å“è³ªé©—è­‰è¦å‰‡)
8. [å¯¦ä½œå»ºè­°](#å¯¦ä½œå»ºè­°)

---

## æ¨¡çµ„æ¦‚è¿°

### åŠŸèƒ½æè¿°
æ–‡ä»¶è§£ææ¨¡çµ„è² è²¬å°‡ DOCX æ ¼å¼çš„æŠ€è¡“äº¤åº•æ›¸è½‰æ›ç‚ºçµæ§‹åŒ–è³‡æ–™ï¼Œæå–æŠ€è¡“è¦ç´ ã€é—œéµè¡“èªå’Œç« ç¯€è³‡è¨Šï¼Œç‚ºå¾ŒçºŒçš„å°ˆåˆ©æª¢ç´¢å’Œæ’°å¯«æä¾›æ¨™æº–åŒ–è¼¸å…¥ã€‚

### æ ¸å¿ƒè·è²¬
1. **æ ¼å¼è½‰æ›**: DOCX â†’ Markdown â†’ çµæ§‹åŒ– JSON
2. **ç« ç¯€è­˜åˆ¥**: è¾¨è­˜æŠ€è¡“é ˜åŸŸã€èƒŒæ™¯æŠ€è¡“ã€ç™¼æ˜å…§å®¹ç­‰ç« ç¯€
3. **è³‡è¨Šæå–**: æå–æŠ€è¡“å•é¡Œã€æŠ€è¡“æ–¹æ¡ˆã€æŠ€è¡“æ•ˆæœ
4. **è¡“èªæå–**: è­˜åˆ¥ä¸¦æå–é—œéµæŠ€è¡“è¡“èª
5. **å“è³ªé©—è­‰**: ç¢ºä¿æå–çš„è³‡è¨Šå®Œæ•´ä¸”ç¬¦åˆæ ¼å¼è¦æ±‚

### æŠ€è¡“ç‰¹é»
- **åŸºæ–¼ Markitdown**: é«˜å“è³ªçš„ DOCX è½‰ Markdown
- **AI å¢å¼·**: ä½¿ç”¨ Claude é€²è¡Œèªæ„ç†è§£
- **çµæ§‹åŒ–è¼¸å‡º**: åš´æ ¼çš„ JSON Schema é©—è­‰
- **éŒ¯èª¤å®¹å¿**: è™•ç†æ ¼å¼ä¸è¦ç¯„çš„æ–‡ä»¶

---

## A2-0: æƒ…å¢ƒåœ–

### åœ–ç¤º

```mermaid
flowchart LR
    A1["A1: ä»‹é¢ç®¡ç†"]
    A3["A3: å°ˆåˆ©æª¢ç´¢"]
    FS["ğŸ“ æª”æ¡ˆç³»çµ±"]
    A2["æ–‡ä»¶è§£æ<br/>Document Parsing"]
    ParseRules["è§£æè¦å‰‡"]
    Schema["JSON Schema"]
    QualityRules["å“è³ªæ¨™æº–"]
    ParsedJSON["ğŸ“Š parsed_info.json"]
    ErrorLog["ğŸ“‹ éŒ¯èª¤æ—¥èªŒ"]

    A1 -->|raw_document.docx| A2
    ParseRules -.æ§åˆ¶.-> A2
    Schema -.æ§åˆ¶.-> A2
    QualityRules -.æ§åˆ¶.-> A2

    A2 -->|çµæ§‹åŒ–è³‡æ–™| ParsedJSON
    A2 -->|è§£æéŒ¯èª¤| ErrorLog

    ParsedJSON --> FS
    ParsedJSON --> A3
    ErrorLog --> FS

    style A2 fill:#C8E6C9,stroke:#388E3C,stroke-width:3px
```

### æƒ…å¢ƒåœ– ICOM åˆ†æ

| è¦ç´  | é …ç›® | è©³ç´°èªªæ˜ |
|------|------|----------|
| **Input (I)** | raw_document.docx | æŠ€è¡“äº¤åº•æ›¸ DOCX æª”æ¡ˆ |
| **Control (C)** | è§£æè¦å‰‡ | ç« ç¯€è­˜åˆ¥æ¨¡å¼ã€é—œéµå­—åˆ—è¡¨ |
| | JSON Schema | è¼¸å‡ºè³‡æ–™çµæ§‹å®šç¾© |
| | å“è³ªæ¨™æº– | å¿…å¡«æ¬„ä½ã€æœ€å°‘è¡“èªæ•¸é‡ |
| **Output (O)** | parsed_info.json | çµæ§‹åŒ–æŠ€è¡“è³‡è¨Š |
| | éŒ¯èª¤æ—¥èªŒ | è§£æå¤±æ•—çš„ç« ç¯€è¨˜éŒ„ |
| **Mechanism (M)** | Markitdown | DOCX è½‰ Markdown å·¥å…· |
| | Claude AI | èªæ„ç†è§£å’Œè³‡è¨Šæå– |
| | input-parser Agent | å°ˆç”¨è§£æ Agent |

---

## A2: é ‚å±¤åŠŸèƒ½åˆ†è§£

### åŠŸèƒ½åˆ†è§£åœ–

```mermaid
flowchart TB
    A21["A2.1<br/>DOCX æ ¼å¼è½‰æ›<br/>Format Conversion"]
    A22["A2.2<br/>ç« ç¯€è­˜åˆ¥èˆ‡åˆ†å‰²<br/>Section Identification"]
    A23["A2.3<br/>é—œéµè³‡è¨Šæå–<br/>Information Extraction"]
    A24["A2.4<br/>çµæ§‹åŒ–è³‡æ–™ç”Ÿæˆ<br/>Data Structuring"]

    Input["ğŸ“„ DOCX æª”æ¡ˆ"] --> A21
    A21 --> Markdown["ğŸ“ Markdown æ–‡æœ¬"]

    Markdown --> A22
    A22 --> Sections["ğŸ“‘ ç« ç¯€æ¸…å–®"]

    Sections --> A23
    A23 --> RawInfo["ğŸ“Š åŸå§‹è³‡è¨Š"]

    RawInfo --> A24
    A24 --> JSON["ğŸ“‹ parsed_info.json"]

    Control["âš™ï¸ æ§åˆ¶:<br/>è§£æè¦å‰‡<br/>Schema å®šç¾©"] -.-> A21
    Control -.-> A22
    Control -.-> A23
    Control -.-> A24

    Mechanism["ğŸ”§ æ©Ÿåˆ¶:<br/>Markitdown<br/>Claude AI"] -.-> A21
    Mechanism -.-> A22
    Mechanism -.-> A23
    Mechanism -.-> A24

    style A21 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
    style A22 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
    style A23 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
    style A24 fill:#BBDEFB,stroke:#1976D2,stroke-width:2px
```

### å­åŠŸèƒ½åˆ—è¡¨

| åŠŸèƒ½ç·¨è™Ÿ | åŠŸèƒ½åç¨± | ä¸»è¦è·è²¬ | é—œéµè¼¸å‡º |
|---------|---------|---------|---------|
| A2.1 | DOCX æ ¼å¼è½‰æ› | å°‡ DOCX è½‰ç‚ºç´”æ–‡æœ¬ Markdown | Markdown æ–‡æœ¬ |
| A2.2 | ç« ç¯€è­˜åˆ¥èˆ‡åˆ†å‰² | è­˜åˆ¥ç« ç¯€çµæ§‹ä¸¦åˆ†å‰²å…§å®¹ | ç« ç¯€å­—å…¸ |
| A2.3 | é—œéµè³‡è¨Šæå– | æå–æŠ€è¡“è¦ç´ ã€è¡“èªã€å¯¦æ–½ä¾‹ | è³‡è¨Šå­—å…¸ |
| A2.4 | çµæ§‹åŒ–è³‡æ–™ç”Ÿæˆ | çµ„ç¹”ç‚ºæ¨™æº– JSON æ ¼å¼ä¸¦é©—è­‰ | parsed_info.json |

---

## å­åŠŸèƒ½è©³ç´°è¨­è¨ˆ

### A2.1: DOCX æ ¼å¼è½‰æ›

#### åŠŸèƒ½æè¿°
ä½¿ç”¨ Markitdown å°‡ DOCX æª”æ¡ˆè½‰æ›ç‚ºçµæ§‹åŒ–çš„ Markdown æ–‡æœ¬ï¼Œä¿ç•™æ¨™é¡Œå±¤ç´šã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰æ ¼å¼è³‡è¨Šã€‚

#### IDEF0 åœ–ç¤º

```
        [è½‰æ›è¦å‰‡, ç·¨ç¢¼æ¨™æº–]
                â†“
[DOCX æª”æ¡ˆ] â†’ [A2.1: æ ¼å¼è½‰æ›] â†’ [Markdown æ–‡æœ¬]
                â†“
         [è½‰æ›æ—¥èªŒ]
                â†‘
        [Markitdown, python-docx]
```

#### ICOM åˆ†æ

| è¦ç´  | é …ç›® | è©³ç´°èªªæ˜ |
|------|------|----------|
| **Input** | DOCX æª”æ¡ˆ | `01_input/raw_document.docx` |
| **Control** | ç·¨ç¢¼æ¨™æº– | UTF-8 ç·¨ç¢¼ |
| | æ ¼å¼ä¿ç•™ | æ¨™é¡Œã€åˆ—è¡¨ã€è¡¨æ ¼ã€ç²—é«” |
| **Output** | Markdown æ–‡æœ¬ | å®Œæ•´çš„æ–‡æª”å…§å®¹ |
| | è½‰æ›æ—¥èªŒ | æˆåŠŸ/å¤±æ•—è¨˜éŒ„ |
| **Mechanism** | Markitdown | Microsoft é–‹æºè½‰æ›å·¥å…· |
| | python-docx | å‚™ç”¨è§£æåº« |

#### å¯¦ä½œç¯„ä¾‹

```python
from markitdown import MarkItDown
from pathlib import Path

def convert_docx_to_markdown(docx_path: Path) -> str:
    """
    å°‡ DOCX è½‰æ›ç‚º Markdown

    Args:
        docx_path: DOCX æª”æ¡ˆè·¯å¾‘

    Returns:
        Markdown æ–‡æœ¬å­—ä¸²
    """
    try:
        converter = MarkItDown()
        result = converter.convert(str(docx_path))
        markdown_text = result.text_content

        # è¨˜éŒ„è½‰æ›æ—¥èªŒ
        log_path = docx_path.parent / "conversion.log"
        log_path.write_text(f"âœ… è½‰æ›æˆåŠŸ\né•·åº¦: {len(markdown_text)} å­—å…ƒ", encoding="utf-8")

        return markdown_text

    except Exception as e:
        error_msg = f"âŒ è½‰æ›å¤±æ•—: {str(e)}"
        log_path = docx_path.parent / "conversion.log"
        log_path.write_text(error_msg, encoding="utf-8")
        raise RuntimeError(error_msg)
```

---

### A2.2: ç« ç¯€è­˜åˆ¥èˆ‡åˆ†å‰²

#### åŠŸèƒ½æè¿°
åŸºæ–¼æ¨™é¡Œå±¤ç´šå’Œé—œéµå­—æ¨¡å¼è­˜åˆ¥æ–‡æª”ç« ç¯€çµæ§‹ï¼Œå°‡å…§å®¹åˆ†å‰²ç‚ºæŠ€è¡“é ˜åŸŸã€èƒŒæ™¯æŠ€è¡“ã€ç™¼æ˜å…§å®¹ç­‰æ¨™æº–ç« ç¯€ã€‚

#### IDEF0 åœ–ç¤º

```
        [ç« ç¯€æ¨¡å¼, é—œéµå­—å­—å…¸]
                â†“
[Markdown] â†’ [A2.2: ç« ç¯€è­˜åˆ¥] â†’ [ç« ç¯€å­—å…¸]
                â†“
         [è­˜åˆ¥ä¿¡å¿ƒåº¦]
                â†‘
        [æ­£å‰‡è¡¨é”å¼, Claude AI]
```

#### ICOM åˆ†æ

| è¦ç´  | é …ç›® | è©³ç´°èªªæ˜ |
|------|------|----------|
| **Input** | Markdown æ–‡æœ¬ | è½‰æ›å¾Œçš„å®Œæ•´æ–‡æœ¬ |
| **Control** | ç« ç¯€æ¨¡å¼ | æ¨™é¡Œé—œéµå­—: "æŠ€è¡“é ˜åŸŸ", "èƒŒæ™¯æŠ€è¡“", "ç™¼æ˜å…§å®¹" |
| | è­˜åˆ¥ç­–ç•¥ | å„ªå…ˆæ¨™é¡ŒåŒ¹é…ï¼Œæ¬¡è¦èªæ„åŒ¹é… |
| **Output** | ç« ç¯€å­—å…¸ | `{"technical_field": "...", "background": "..."}` |
| | è­˜åˆ¥ä¿¡å¿ƒåº¦ | æ¯å€‹ç« ç¯€çš„åŒ¹é…åˆ†æ•¸ |
| **Mechanism** | æ­£å‰‡è¡¨é”å¼ | æ¨¡å¼åŒ¹é… |
| | Claude AI | èªæ„ç†è§£ (ç•¶æ¨¡å¼åŒ¹é…å¤±æ•—æ™‚) |

#### ç« ç¯€è­˜åˆ¥æµç¨‹

```mermaid
flowchart TD
    Start([Markdown æ–‡æœ¬]) --> SplitHeadings[æŒ‰æ¨™é¡Œåˆ†å‰²]
    SplitHeadings --> CheckH1{æœ‰ H1 æ¨™é¡Œ?}

    CheckH1 -->|æ˜¯| MatchKeywords[åŒ¹é…ç« ç¯€é—œéµå­—]
    CheckH1 -->|å¦| FallbackAI[ä½¿ç”¨ AI èªæ„è­˜åˆ¥]

    MatchKeywords --> Matched{åŒ¹é…æˆåŠŸ?}
    Matched -->|æ˜¯| BuildDict[å»ºç«‹ç« ç¯€å­—å…¸]
    Matched -->|å¦| FallbackAI

    FallbackAI --> AIPrompt[Claude: è­˜åˆ¥ç« ç¯€çµæ§‹]
    AIPrompt --> BuildDict

    BuildDict --> Validate{ç« ç¯€å®Œæ•´?}
    Validate -->|å¦| AddPlaceholder[è£œå……ç©ºç™½ç« ç¯€]
    Validate -->|æ˜¯| Output([è¼¸å‡ºç« ç¯€å­—å…¸])
    AddPlaceholder --> Output

    style Start fill:#E8F5E9,stroke:#66BB6A
    style Output fill:#C8E6C9,stroke:#388E3C
    style FallbackAI fill:#FFF9C4,stroke:#F57C00
```

#### å¯¦ä½œç¯„ä¾‹

```python
import re
from typing import Dict

SECTION_KEYWORDS = {
    "technical_field": ["æŠ€è¡“é ˜åŸŸ", "æ‰€å±¬æŠ€è¡“é ˜åŸŸ", "technical field"],
    "background": ["èƒŒæ™¯æŠ€è¡“", "ç¾æœ‰æŠ€è¡“", "prior art", "background"],
    "invention_content": ["ç™¼æ˜å…§å®¹", "æŠ€è¡“æ–¹æ¡ˆ", "invention", "summary"],
    "embodiments": ["å…·é«”å¯¦æ–½æ–¹å¼", "å¯¦æ–½ä¾‹", "embodiment", "detailed description"],
    "advantages": ["æœ‰ç›Šæ•ˆæœ", "å„ªé»", "advantageous effects"]
}

def identify_sections(markdown_text: str) -> Dict[str, str]:
    """
    è­˜åˆ¥ç« ç¯€çµæ§‹

    Args:
        markdown_text: Markdown æ–‡æœ¬

    Returns:
        ç« ç¯€å­—å…¸
    """
    sections = {}

    # æŒ‰æ¨™é¡Œåˆ†å‰²
    lines = markdown_text.split("\n")
    current_section = None
    current_content = []

    for line in lines:
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¨™é¡Œ
        if line.startswith("#"):
            # å„²å­˜å‰ä¸€å€‹ç« ç¯€
            if current_section:
                sections[current_section] = "\n".join(current_content).strip()

            # è­˜åˆ¥æ–°ç« ç¯€
            current_section = match_section_type(line)
            current_content = []
        else:
            current_content.append(line)

    # å„²å­˜æœ€å¾Œä¸€å€‹ç« ç¯€
    if current_section:
        sections[current_section] = "\n".join(current_content).strip()

    # é©—è­‰å¿…è¦ç« ç¯€
    required_sections = ["technical_field", "background", "invention_content"]
    for section in required_sections:
        if section not in sections:
            sections[section] = ""

    return sections

def match_section_type(heading: str) -> str:
    """åŒ¹é…ç« ç¯€é¡å‹"""
    heading_lower = heading.lower().strip("#").strip()

    for section_key, keywords in SECTION_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in heading_lower:
                return section_key

    return "other"
```

---

### A2.3: é—œéµè³‡è¨Šæå–

#### åŠŸèƒ½æè¿°
å¾å„ç« ç¯€ä¸­æå–æŠ€è¡“å•é¡Œã€æŠ€è¡“æ–¹æ¡ˆã€æŠ€è¡“æ•ˆæœã€é—œéµè¡“èªç­‰æ ¸å¿ƒè³‡è¨Šï¼Œä½¿ç”¨ AI é€²è¡Œèªæ„ç†è§£å’Œçµæ§‹åŒ–ã€‚

#### IDEF0 åœ–ç¤º

```
        [æå–æ¨¡æ¿, Prompt è¦å‰‡]
                â†“
[ç« ç¯€å­—å…¸] â†’ [A2.3: è³‡è¨Šæå–] â†’ [çµæ§‹åŒ–è³‡è¨Š]
                â†“
         [æå–æ—¥èªŒ]
                â†‘
        [Claude AI, NER æ¨¡å‹]
```

#### ICOM åˆ†æ

| è¦ç´  | é …ç›® | è©³ç´°èªªæ˜ |
|------|------|----------|
| **Input** | ç« ç¯€å­—å…¸ | å„ç« ç¯€çš„æ–‡æœ¬å…§å®¹ |
| **Control** | æå–æ¨¡æ¿ | å„ç« ç¯€çš„æå–è¦å‰‡ |
| | Prompt å·¥ç¨‹ | AI æç¤ºè©è¨­è¨ˆ |
| **Output** | æŠ€è¡“å•é¡Œåˆ—è¡¨ | èƒŒæ™¯æŠ€è¡“ä¸­çš„å•é¡Œ |
| | æŠ€è¡“æ–¹æ¡ˆå­—å…¸ | æ ¸å¿ƒæ€æƒ³ã€é—œéµç‰¹å¾µ |
| | æŠ€è¡“æ•ˆæœåˆ—è¡¨ | æœ‰ç›Šæ•ˆæœ |
| | é—œéµè¡“èªåˆ—è¡¨ | æŠ€è¡“è¡“èªåŠå®šç¾© |
| **Mechanism** | Claude AI | èªæ„ç†è§£å’Œæå– |
| | spaCy/NLTK | è¼”åŠ© NER (Named Entity Recognition) |

#### æå– Prompt ç¯„ä¾‹

```python
EXTRACTION_PROMPTS = {
    "technical_problems": """
è«‹å¾ä»¥ä¸‹èƒŒæ™¯æŠ€è¡“ç« ç¯€ä¸­æå–æŠ€è¡“å•é¡Œ:

{background_text}

è¼¸å‡ºæ ¼å¼ (JSON):
{{
  "problems": [
    "å•é¡Œ1æè¿°",
    "å•é¡Œ2æè¿°"
  ]
}}
""",

    "technical_solution": """
è«‹å¾ä»¥ä¸‹ç™¼æ˜å…§å®¹ç« ç¯€ä¸­æå–æŠ€è¡“æ–¹æ¡ˆ:

{invention_content}

è¼¸å‡ºæ ¼å¼ (JSON):
{{
  "core_idea": "æ ¸å¿ƒæŠ€è¡“æ€æƒ³ä¸€å¥è©±æ¦‚æ‹¬",
  "key_features": ["ç‰¹å¾µ1", "ç‰¹å¾µ2", "ç‰¹å¾µ3"],
  "implementation": "å¯¦æ–½æ–¹å¼æ¦‚è¿°"
}}
""",

    "key_terms": """
è«‹å¾ä»¥ä¸‹æ–‡æœ¬ä¸­æå–é—œéµæŠ€è¡“è¡“èª:

{full_text}

è¼¸å‡ºæ ¼å¼ (JSON):
{{
  "terms": [
    {{"term": "è¡“èª1", "definition": "å®šç¾©1"}},
    {{"term": "è¡“èª2", "definition": "å®šç¾©2"}}
  ]
}}
"""
}

async def extract_information(sections: Dict[str, str], claude_client) -> Dict:
    """
    æå–é—œéµè³‡è¨Š

    Args:
        sections: ç« ç¯€å­—å…¸
        claude_client: Claude AI å®¢æˆ¶ç«¯

    Returns:
        æå–çš„çµæ§‹åŒ–è³‡è¨Š
    """
    extracted = {}

    # 1. æå–æŠ€è¡“å•é¡Œ
    if sections.get("background"):
        prompt = EXTRACTION_PROMPTS["technical_problems"].format(
            background_text=sections["background"]
        )
        response = await claude_client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )
        extracted["problems"] = parse_json_response(response.content[0].text)

    # 2. æå–æŠ€è¡“æ–¹æ¡ˆ
    if sections.get("invention_content"):
        prompt = EXTRACTION_PROMPTS["technical_solution"].format(
            invention_content=sections["invention_content"]
        )
        response = await claude_client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2048,
            messages=[{"role": "user", "content": prompt}]
        )
        extracted["solution"] = parse_json_response(response.content[0].text)

    # 3. æå–é—œéµè¡“èª
    full_text = "\n\n".join(sections.values())
    prompt = EXTRACTION_PROMPTS["key_terms"].format(full_text=full_text[:5000])
    response = await claude_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2048,
        messages=[{"role": "user", "content": prompt}]
    )
    extracted["key_terms"] = parse_json_response(response.content[0].text)

    return extracted
```

---

### A2.4: çµæ§‹åŒ–è³‡æ–™ç”Ÿæˆ

#### åŠŸèƒ½æè¿°
å°‡æå–çš„è³‡è¨Šçµ„ç¹”ç‚ºæ¨™æº– JSON æ ¼å¼ï¼Œé€²è¡Œ Schema é©—è­‰ï¼Œä¸¦å„²å­˜ç‚º `parsed_info.json`ã€‚

#### IDEF0 åœ–ç¤º

```
        [JSON Schema, é©—è­‰è¦å‰‡]
                â†“
[åŸå§‹è³‡è¨Š] â†’ [A2.4: è³‡æ–™ç”Ÿæˆ] â†’ [parsed_info.json]
                â†“
         [é©—è­‰å ±å‘Š]
                â†‘
        [Pydantic, jsonschema]
```

#### ICOM åˆ†æ

| è¦ç´  | é …ç›® | è©³ç´°èªªæ˜ |
|------|------|----------|
| **Input** | åŸå§‹è³‡è¨Šå­—å…¸ | å‰ä¸€æ­¥æå–çš„æ‰€æœ‰è³‡è¨Š |
| | æ–‡æª”å…ƒè³‡æ–™ | æª”æ¡ˆåç¨±ã€å»ºç«‹æ™‚é–“ |
| **Control** | JSON Schema | æ¨™æº–è³‡æ–™çµæ§‹å®šç¾© |
| | é©—è­‰è¦å‰‡ | å¿…å¡«æ¬„ä½ã€è³‡æ–™é¡å‹ã€å€¼ç¯„åœ |
| **Output** | parsed_info.json | å®Œæ•´çš„çµæ§‹åŒ–æª”æ¡ˆ |
| | é©—è­‰å ±å‘Š | é€šé/å¤±æ•—åŠéŒ¯èª¤è©³æƒ… |
| **Mechanism** | Pydantic | Python è³‡æ–™é©—è­‰ |
| | jsonschema | JSON Schema é©—è­‰ |

#### JSON Schema å®šç¾©

```python
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional
from datetime import datetime

class TechnicalProblem(BaseModel):
    """æŠ€è¡“å•é¡Œ"""
    description: str = Field(..., min_length=10, max_length=500)

class TechnicalSolution(BaseModel):
    """æŠ€è¡“æ–¹æ¡ˆ"""
    core_idea: str = Field(..., min_length=20)
    key_features: List[str] = Field(..., min_items=3, max_items=10)
    implementation: str = Field(..., min_length=50)

class KeyTerm(BaseModel):
    """é—œéµè¡“èª"""
    term: str = Field(..., min_length=2, max_length=50)
    definition: str = Field(..., min_length=5, max_length=300)

class Embodiment(BaseModel):
    """å¯¦æ–½ä¾‹"""
    title: str
    description: str = Field(..., min_length=100)
    figures: List[str] = []

class Metadata(BaseModel):
    """å…ƒè³‡æ–™"""
    title: str = Field(default="", description="å°ˆåˆ©åç¨±")
    author: str = Field(default="", description="ç™¼æ˜äºº")
    date: str = Field(default_factory=lambda: datetime.now().isoformat())
    uuid: str = Field(..., description="æœƒè©± UUID")

class ParsedInfo(BaseModel):
    """å®Œæ•´çš„è§£æçµæœ"""
    metadata: Metadata
    technical_field: str = Field(..., min_length=10, description="æŠ€è¡“é ˜åŸŸ")
    background: Dict[str, List[str]] = Field(
        default_factory=lambda: {"problems": [], "prior_art": []},
        description="èƒŒæ™¯æŠ€è¡“"
    )
    technical_solution: TechnicalSolution
    advantages: List[str] = Field(default_factory=list, description="æœ‰ç›Šæ•ˆæœ")
    embodiments: List[Embodiment] = Field(default_factory=list)
    key_terms: List[KeyTerm] = Field(..., min_items=5, description="é—œéµè¡“èª")

    @validator('key_terms')
    def validate_key_terms(cls, v):
        if len(v) < 5:
            raise ValueError("è‡³å°‘éœ€è¦ 5 å€‹é—œéµè¡“èª")
        return v

    class Config:
        json_schema_extra = {
            "example": {
                "metadata": {
                    "title": "åŸºæ–¼ AI çš„å°ˆåˆ©æ’°å¯«ç³»çµ±",
                    "author": "å¼µä¸‰",
                    "date": "2025-10-30",
                    "uuid": "abc123"
                },
                "technical_field": "äººå·¥æ™ºæ…§è¼”åŠ©å¯«ä½œæŠ€è¡“é ˜åŸŸ",
                "background": {
                    "problems": ["ç¾æœ‰å°ˆåˆ©æ’°å¯«æ•ˆç‡ä½", "å°ˆæ¥­äººæ‰çŸ­ç¼º"],
                    "prior_art": ["äººå·¥æ’°å¯«", "ç°¡å–®æ¨¡æ¿ç”Ÿæˆ"]
                },
                "technical_solution": {
                    "core_idea": "ä½¿ç”¨å¤§èªè¨€æ¨¡å‹è‡ªå‹•ç”Ÿæˆå°ˆåˆ©æ–‡ä»¶",
                    "key_features": ["å¤š Agent å”ä½œ", "çµæ§‹åŒ–è³‡æ–™æµ", "å“è³ªé©—è­‰"],
                    "implementation": "åŸºæ–¼ Claude API æ§‹å»ºæµç¨‹æ§åˆ¶ç³»çµ±"
                },
                "advantages": ["æé«˜æ•ˆç‡", "é™ä½æˆæœ¬", "ä¿è­‰å“è³ª"],
                "embodiments": [],
                "key_terms": [
                    {"term": "å¤§èªè¨€æ¨¡å‹", "definition": "åŸºæ–¼ Transformer çš„ç”Ÿæˆå¼ AI"},
                    {"term": "Agent", "definition": "å…·æœ‰ç‰¹å®šåŠŸèƒ½çš„æ™ºèƒ½ä»£ç†"}
                ]
            }
        }

def generate_structured_data(
    sections: Dict[str, str],
    extracted_info: Dict,
    session_uuid: str,
    output_path: Path
) -> ParsedInfo:
    """
    ç”Ÿæˆä¸¦é©—è­‰çµæ§‹åŒ–è³‡æ–™

    Args:
        sections: ç« ç¯€å­—å…¸
        extracted_info: æå–çš„è³‡è¨Š
        session_uuid: æœƒè©± UUID
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘

    Returns:
        é©—è­‰å¾Œçš„ ParsedInfo ç‰©ä»¶
    """
    try:
        # å»ºç«‹çµæ§‹åŒ–è³‡æ–™
        parsed_data = ParsedInfo(
            metadata=Metadata(uuid=session_uuid),
            technical_field=sections.get("technical_field", ""),
            background={
                "problems": extracted_info.get("problems", []),
                "prior_art": []
            },
            technical_solution=TechnicalSolution(**extracted_info.get("solution", {})),
            key_terms=[KeyTerm(**term) for term in extracted_info.get("key_terms", {}).get("terms", [])]
        )

        # å„²å­˜ç‚º JSON
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(parsed_data.model_dump_json(indent=2, exclude_none=True))

        return parsed_data

    except ValidationError as e:
        # é©—è­‰å¤±æ•—
        error_path = output_path.parent / "validation_error.log"
        error_path.write_text(str(e), encoding="utf-8")
        raise
```

---

## è§£ææ¼”ç®—æ³•æµç¨‹

### å®Œæ•´æµç¨‹

```mermaid
flowchart TD
    Start([DOCX æª”æ¡ˆ]) --> A21[A2.1: æ ¼å¼è½‰æ›]
    A21 --> CheckMD{Markdown æœ‰æ•ˆ?}
    CheckMD -->|å¦| Error1[è¨˜éŒ„éŒ¯èª¤ä¸¦çµ‚æ­¢]
    CheckMD -->|æ˜¯| A22[A2.2: ç« ç¯€è­˜åˆ¥]

    A22 --> CheckSections{ç« ç¯€å®Œæ•´?}
    CheckSections -->|å¦| AIFallback[ä½¿ç”¨ AI è£œå……è­˜åˆ¥]
    CheckSections -->|æ˜¯| A23[A2.3: è³‡è¨Šæå–]
    AIFallback --> A23

    A23 --> Parallel{å¹³è¡Œæå–}
    Parallel --> Extract1[æå–æŠ€è¡“å•é¡Œ]
    Parallel --> Extract2[æå–æŠ€è¡“æ–¹æ¡ˆ]
    Parallel --> Extract3[æå–é—œéµè¡“èª]

    Extract1 --> Merge[åˆä½µæå–çµæœ]
    Extract2 --> Merge
    Extract3 --> Merge

    Merge --> A24[A2.4: è³‡æ–™ç”Ÿæˆ]
    A24 --> Validate{Schema é©—è­‰}

    Validate -->|å¤±æ•—| Error2[è¨˜éŒ„é©—è­‰éŒ¯èª¤]
    Error2 --> Retry{é‡è©¦æ¬¡æ•¸ < 3?}
    Retry -->|æ˜¯| A23
    Retry -->|å¦| FinalError[è¼¸å‡ºéŒ¯èª¤å ±å‘Š]

    Validate -->|æˆåŠŸ| SaveJSON[å„²å­˜ parsed_info.json]
    SaveJSON --> End([å®Œæˆ])

    Error1 --> FinalError
    FinalError --> End

    style Start fill:#E8F5E9,stroke:#66BB6A
    style End fill:#C8E6C9,stroke:#388E3C
    style Error1 fill:#FFCDD2,stroke:#C62828
    style Error2 fill:#FFCDD2,stroke:#C62828
    style FinalError fill:#FFCDD2,stroke:#C62828
    style Validate fill:#FFF9C4,stroke:#F57C00
```

---

## JSON Schema å®šç¾©

å®Œæ•´çš„ JSON Schema å®šç¾©è«‹åƒè€ƒä¸Šæ–‡ A2.4 çš„ Pydantic æ¨¡å‹ã€‚

---

## å“è³ªé©—è­‰è¦å‰‡

### é©—è­‰æ¸…å–®

| é©—è­‰é …ç›® | è¦å‰‡ | éŒ¯èª¤è™•ç† |
|---------|------|---------|
| **å¿…å¡«æ¬„ä½** | technical_field, technical_solution, key_terms å¿…é ˆå­˜åœ¨ | æç¤ºç¼ºå°‘æ¬„ä½ï¼Œæ‹’çµ•ç”Ÿæˆ |
| **å­—æ•¸é™åˆ¶** | technical_field >= 10 å­— | è­¦å‘Šä¸¦è¦æ±‚è£œå…… |
| **è¡“èªæ•¸é‡** | key_terms >= 5 å€‹ | AI è£œå……æå– |
| **æŠ€è¡“ç‰¹å¾µ** | key_features >= 3 å€‹ | AI è£œå……æå– |
| **JSON æ ¼å¼** | ç¬¦åˆ JSON èªæ³• | è‡ªå‹•ä¿®å¾©æˆ–å ±éŒ¯ |
| **ç·¨ç¢¼æ ¼å¼** | UTF-8 | è‡ªå‹•è½‰æ› |

### å“è³ªè©•åˆ†

```python
def calculate_quality_score(parsed_info: ParsedInfo) -> float:
    """
    è¨ˆç®—è§£æå“è³ªåˆ†æ•¸ (0-100)

    è©•åˆ†æ¨™æº–:
    - ç« ç¯€å®Œæ•´æ€§ (30%)
    - é—œéµè¡“èªæ•¸é‡å’Œå“è³ª (25%)
    - æŠ€è¡“æ–¹æ¡ˆè©³ç´°åº¦ (25%)
    - å¯¦æ–½ä¾‹æ•¸é‡ (10%)
    - è³‡æ–™å®Œæ•´æ€§ (10%)
    """
    score = 0.0

    # 1. ç« ç¯€å®Œæ•´æ€§ (30 åˆ†)
    required_fields = ["technical_field", "technical_solution", "key_terms"]
    filled_fields = sum(1 for field in required_fields if getattr(parsed_info, field))
    score += (filled_fields / len(required_fields)) * 30

    # 2. é—œéµè¡“èª (25 åˆ†)
    term_count = len(parsed_info.key_terms)
    score += min(term_count / 10, 1.0) * 25  # 10 å€‹è¡“èªå¾—æ»¿åˆ†

    # 3. æŠ€è¡“æ–¹æ¡ˆè©³ç´°åº¦ (25 åˆ†)
    feature_count = len(parsed_info.technical_solution.key_features)
    implementation_length = len(parsed_info.technical_solution.implementation)
    score += min(feature_count / 5, 1.0) * 15  # 5 å€‹ç‰¹å¾µå¾— 15 åˆ†
    score += min(implementation_length / 200, 1.0) * 10  # 200 å­—å¾— 10 åˆ†

    # 4. å¯¦æ–½ä¾‹æ•¸é‡ (10 åˆ†)
    embodiment_count = len(parsed_info.embodiments)
    score += min(embodiment_count / 3, 1.0) * 10  # 3 å€‹å¯¦æ–½ä¾‹å¾—æ»¿åˆ†

    # 5. è³‡æ–™å®Œæ•´æ€§ (10 åˆ†)
    has_problems = len(parsed_info.background.get("problems", [])) > 0
    has_advantages = len(parsed_info.advantages) > 0
    score += (has_problems + has_advantages) * 5

    return round(score, 2)
```

---

## å¯¦ä½œå»ºè­°

### æŠ€è¡“é¸å‹

```python
# æ ¸å¿ƒä¾è³´
dependencies = [
    "markitdown>=0.1.0",          # DOCX è½‰ Markdown
    "anthropic>=0.18.0",          # Claude AI SDK
    "pydantic>=2.0.0",            # è³‡æ–™é©—è­‰
    "python-docx>=0.8.11",        # å‚™ç”¨ DOCX è§£æ
    "spacy>=3.7.0",               # NLP å·¥å…·
    "jsonschema>=4.20.0",         # JSON Schema é©—è­‰
]

# å¯é¸ä¾è³´
optional_dependencies = [
    "tiktoken>=0.5.0",            # Token è¨ˆæ•¸
    "langchain>=0.1.0",           # Agent æ¡†æ¶
]
```

### ç¨‹å¼ç¢¼çµæ§‹

```
src/
â”œâ”€â”€ parsing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ converter.py           # A2.1: æ ¼å¼è½‰æ›
â”‚   â”œâ”€â”€ section_identifier.py  # A2.2: ç« ç¯€è­˜åˆ¥
â”‚   â”œâ”€â”€ extractor.py           # A2.3: è³‡è¨Šæå–
â”‚   â”œâ”€â”€ structurer.py          # A2.4: è³‡æ–™ç”Ÿæˆ
â”‚   â”œâ”€â”€ models.py              # Pydantic æ¨¡å‹
â”‚   â”œâ”€â”€ prompts.py             # AI Prompts
â”‚   â””â”€â”€ validators.py          # é©—è­‰å™¨
â””â”€â”€ tests/
    â””â”€â”€ parsing/
        â”œâ”€â”€ test_converter.py
        â”œâ”€â”€ test_extractor.py
        â””â”€â”€ test_integration.py
```

### æ¸¬è©¦ç­–ç•¥

```python
# tests/parsing/test_extractor.py
import pytest
from parsing.extractor import extract_information

@pytest.fixture
def sample_sections():
    return {
        "technical_field": "äººå·¥æ™ºæ…§æŠ€è¡“é ˜åŸŸ",
        "background": "ç¾æœ‰æŠ€è¡“å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„å•é¡Œ...",
        "invention_content": "æœ¬ç™¼æ˜æä¾›ä¸€ç¨®åŸºæ–¼ AI çš„è‡ªå‹•åŒ–ç³»çµ±..."
    }

@pytest.mark.asyncio
async def test_extract_technical_problems(sample_sections, claude_client):
    """æ¸¬è©¦æŠ€è¡“å•é¡Œæå–"""
    result = await extract_information(sample_sections, claude_client)

    assert "problems" in result
    assert len(result["problems"]) > 0
    assert all(len(problem) >= 10 for problem in result["problems"])

@pytest.mark.asyncio
async def test_extract_key_terms(sample_sections, claude_client):
    """æ¸¬è©¦é—œéµè¡“èªæå–"""
    result = await extract_information(sample_sections, claude_client)

    assert "key_terms" in result
    assert len(result["key_terms"]["terms"]) >= 5

    for term in result["key_terms"]["terms"]:
        assert "term" in term
        assert "definition" in term
```

---

## ç¸½çµ

### æ¨¡çµ„ç‰¹é»

âœ… **æ™ºèƒ½åŒ–**: AI å¢å¼·çš„èªæ„ç†è§£
âœ… **çµæ§‹åŒ–**: åš´æ ¼çš„ JSON Schema é©—è­‰
âœ… **å®¹éŒ¯æ€§**: å¤šå±¤ç´šéŒ¯èª¤è™•ç†å’Œé‡è©¦
âœ… **å¯è¿½æº¯**: å®Œæ•´çš„æ—¥èªŒå’Œé©—è­‰å ±å‘Š
âœ… **é«˜å“è³ª**: å“è³ªè©•åˆ†å’Œé©—è­‰æ©Ÿåˆ¶

### é—œéµæŒ‡æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | æ¸¬é‡æ–¹æ³• |
|-----|-------|---------|
| è§£ææˆåŠŸç‡ | > 95% | æˆåŠŸè§£ææ•¸ / ç¸½æª”æ¡ˆæ•¸ |
| ç« ç¯€è­˜åˆ¥æº–ç¢ºç‡ | > 90% | æ­£ç¢ºè­˜åˆ¥ç« ç¯€æ•¸ / ç¸½ç« ç¯€æ•¸ |
| è¡“èªæå–æ•¸é‡ | >= 10 å€‹ | å¹³å‡æ¯ä»½æ–‡æª”çš„è¡“èªæ•¸ |
| è™•ç†æ™‚é–“ | < 2 åˆ†é˜ | å¾è¼¸å…¥åˆ°è¼¸å‡ºçš„æ™‚é–“ |
| å“è³ªè©•åˆ† | > 80 åˆ† | å¹³å‡å“è³ªåˆ†æ•¸ |

---

**æ–‡ä»¶çµæŸ**
